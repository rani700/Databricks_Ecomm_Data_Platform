{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb49797c-2a69-4424-a8fa-e2e739078f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Shared_Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b16de1-cd1d-49e7-a7a9-1fab305edddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "# Ensure Schema Exists\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ecomm_data_project.audit\")\n",
    "\n",
    "# Define Schema\n",
    "audit_schema = StructType([\n",
    "    StructField(\"run_id\", StringType(), True),\n",
    "    StructField(\"job_name\", StringType(), True),\n",
    "    StructField(\"layer\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"records_processed\", LongType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"error_message\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create the Table as a managed Delta table in Unity Catalog\n",
    "empty_df = spark.createDataFrame([], audit_schema)\n",
    "(\n",
    "    empty_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"ignore\")\n",
    "    .saveAsTable(\"ecomm_data_project.audit.pipeline_logs\")\n",
    ")\n",
    "\n",
    "display(spark.table(\"ecomm_data_project.audit.pipeline_logs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de64db67-b167-420b-a423-c0f5c556d2f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40a2b44-9863-4100-bdfb-305e3c451b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 1. GET RUN ID\n",
    "try:\n",
    "    run_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().runId().get()\n",
    "except:\n",
    "    run_id = \"manual_run\"\n",
    "print(f\"Current Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce09dbec-99a8-4b57-9d62-2759d4c07d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. SETUP WIDGETS & PATHS\n",
    "dbutils.widgets.text(\"process_type\", \"all\")\n",
    "process_type = dbutils.widgets.get(\"process_type\")\n",
    "\n",
    "base_path = \"/Volumes/ecomm_data_project/ecomm_raw/landing_zone\"\n",
    "checkpoint_base = \"/Volumes/ecomm_data_project/bronze/checkpoints\"\n",
    "\n",
    "print(f\"Pipeline running for mode: {process_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aedbd793-7378-4eef-87fa-b5dc4aa3de35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PATH A: PROCESS USER DATA (With Logging)\n",
    "# ==========================================\n",
    "if process_type in [\"users\", \"all\"]:\n",
    "    job_name = \"Bronze_Ingest_Users\"\n",
    "    try:\n",
    "        print(f\">>> [START] {job_name}...\")\n",
    "        \n",
    "        # Run Auto Loader\n",
    "        query = (spark.readStream\n",
    "                  .format(\"cloudFiles\")\n",
    "                  .option(\"cloudFiles.format\", \"parquet\")\n",
    "                  .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_base}/users_schema\")\n",
    "                  .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") \n",
    "                  .load(f\"{base_path}/users-raw-2\")\n",
    "                  .writeStream\n",
    "                  .option(\"checkpointLocation\", f\"{checkpoint_base}/users\")\n",
    "                  .option(\"mergeSchema\", \"true\") \n",
    "                  .trigger(availableNow=True) # Process all data then stop\n",
    "                  .toTable(\"ecomm_data_project.bronze.users\"))\n",
    "        \n",
    "        # Wait for the stream to actually finish writing\n",
    "        query.awaitTermination()\n",
    "        \n",
    "        # Metric: Get Total Row Count (Simple Proxy for success)\n",
    "        total_count = spark.read.table(\"ecomm_data_project.bronze.users\").count()\n",
    "        \n",
    "        # LOG SUCCESS\n",
    "        log_pipeline_run(run_id, job_name, \"Bronze\", \"SUCCESS\", total_count)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # LOG FAILURE\n",
    "        print(f\"!!! Error in {job_name}: {e}\")\n",
    "        log_pipeline_run(run_id, job_name, \"Bronze\", \"FAILED\", 0, str(e))\n",
    "        raise e # Re-raise error so Databricks Job knows it failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28abc459-f834-4dd4-beed-9593da7a78b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PATH B: PROCESS REFERENCE DATA (With Logging)\n",
    "# ==========================================\n",
    "if process_type in [\"reference\", \"all\"]:\n",
    "    job_name = \"Bronze_Ingest_Reference\"\n",
    "    try:\n",
    "        print(f\">>> [START] {job_name}...\")\n",
    "        \n",
    "        ref_tables = [\"buyers\", \"sellers\", \"countries\"]\n",
    "        total_rows_affected = 0\n",
    "        \n",
    "        for table in ref_tables:\n",
    "            df = spark.read.parquet(f\"{base_path}/{table}-raw-2\")\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"ecomm_data_project.bronze.{table}\")\n",
    "            total_rows_affected += df.count()\n",
    "            \n",
    "        # LOG SUCCESS\n",
    "        log_pipeline_run(run_id, job_name, \"Bronze\", \"SUCCESS\", total_rows_affected)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # LOG FAILURE\n",
    "        log_pipeline_run(run_id, job_name, \"Bronze\", \"FAILED\", 0, str(e))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e467b0e1-bc7d-4fee-b2bd-975b62e96f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7276245788884340,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_Layer",
   "widgets": {
    "process_type": {
     "currentValue": "all",
     "nuid": "0e731def-c6be-46fc-897d-a43b98c77132",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "all",
      "label": null,
      "name": "process_type",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "all",
      "label": null,
      "name": "process_type",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
