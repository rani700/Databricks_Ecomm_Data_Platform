{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adaa848d-1923-4d77-8233-7b39239eec5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Shared_Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf40007-be63-47f4-a998-9a388f30d1f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# 0. SETUP & UTILITIES\n",
    "# MAGIC %run ./Shared_Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69018a27-42e4-45ec-b8bf-b4c35a4ddde8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GET RUN ID \n",
    "try:\n",
    "    # This magic line grabs the unique ID of this current execution\n",
    "    run_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().runId().get()\n",
    "except:\n",
    "    run_id = \"manual_run_gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c1f596-a125-490b-8097-112b13005964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TASK 1: THE \"USER 360\" ONE BIG TABLE (With AI Metadata)\n",
    "# ==========================================\n",
    "job_name_obt = \"Gold_One_Big_Table\"\n",
    "try:\n",
    "    print(f\">>> [START] {job_name_obt}...\")\n",
    "\n",
    "    # 1. READ & FILTER\n",
    "    u = spark.read.table(\"ecomm_data_project.silver.users\").filter(\"is_current = true\").alias(\"u\")\n",
    "    b = spark.read.table(\"ecomm_data_project.silver.buyers\").alias(\"b\")\n",
    "    s = spark.read.table(\"ecomm_data_project.silver.sellers\").alias(\"s\")\n",
    "    c = spark.read.table(\"ecomm_data_project.silver.countries\").alias(\"c\")\n",
    "\n",
    "    # 2. JOIN LOGIC\n",
    "    gold_final_df = u \\\n",
    "        .join(c, u.countrycode == c.country, \"left\") \\\n",
    "        .join(b, c.country == b.country, \"left\") \\\n",
    "        .join(s, c.country == s.country, \"left\") \\\n",
    "        .select(\n",
    "            F.col(\"u.identifierHash\").alias(\"User_ID\"),\n",
    "            F.col(\"u.countrycode\").alias(\"Country\"),\n",
    "            F.col(\"u.gender\").alias(\"User_Gender\"),\n",
    "            F.col(\"u.productsSold\").alias(\"User_ProductsSold\"),\n",
    "            F.col(\"u.productsWished\").alias(\"User_ProductsWished\"),\n",
    "            F.col(\"u.account_age_years\").alias(\"User_AccountAge\"),\n",
    "            F.col(\"s.nbsellers\").alias(\"Country_TotalSellers\"),\n",
    "            F.col(\"b.buyers\").alias(\"Country_TotalBuyers\"),\n",
    "            F.col(\"b.female_to_male_ratio\").alias(\"Country_BuyerRatio\")\n",
    "        )\n",
    "\n",
    "    # 3. WRITE WITH METADATA (The MNC Way)\n",
    "    # We use 'overwrite' to ensure the schema and comments are fresh\n",
    "    gold_final_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"ecomm_data_project.gold.ecom_one_big_table\")\n",
    "\n",
    "    # 4. SET TABLE & COLUMN COMMENTS FOR THE AI AGENT\n",
    "    # This acts as the 'Documentation' the LLM reads to understand your data\n",
    "    spark.sql(\"COMMENT ON TABLE ecomm_data_project.gold.ecom_one_big_table IS 'Primary user 360 table containing demographics and country-level sales metrics for e-commerce analysis.'\")\n",
    "    \n",
    "    spark.sql(\"ALTER TABLE ecomm_data_project.gold.ecom_one_big_table ALTER COLUMN User_ProductsSold COMMENT 'Total count of successful sales made by this user.'\")\n",
    "    spark.sql(\"ALTER TABLE ecomm_data_project.gold.ecom_one_big_table ALTER COLUMN Country_BuyerRatio COMMENT 'The ratio of female to male buyers in the users country. Used for gender-based market analysis.'\")\n",
    "    spark.sql(\"ALTER TABLE ecomm_data_project.gold.ecom_one_big_table ALTER COLUMN User_AccountAge COMMENT 'The number of years the user has been registered on the platform.'\")\n",
    "\n",
    "    log_pipeline_run(run_id, job_name_obt, \"Gold\", \"SUCCESS\", gold_final_df.count())\n",
    "    print(f\">>> [SUCCESS] {job_name_obt} complete with AI Metadata.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_pipeline_run(run_id, job_name_obt, \"Gold\", \"FAILED\", 0, str(e))\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "252b04cc-4b8a-41d7-a284-d1d516b2c86f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TASK 2: USER DEMOGRAPHICS AGGREGATION\n",
    "# ==========================================\n",
    "job_name_demo = \"Gold_User_Demographics\"\n",
    "try:\n",
    "    print(f\">>> [START] {job_name_demo}...\")\n",
    "    \n",
    "    # Aggregate only active users\n",
    "    demo_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            countrycode,\n",
    "            gender,\n",
    "            COUNT(identifierHash) as total_users,\n",
    "            AVG(account_age_years) as avg_account_age\n",
    "        FROM ecomm_data_project.silver.users\n",
    "        WHERE is_current = true\n",
    "        GROUP BY countrycode, gender\n",
    "    \"\"\")\n",
    "    \n",
    "    demo_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecomm_data_project.gold.user_demographics\")\n",
    "    \n",
    "    # Log Success\n",
    "    log_pipeline_run(run_id, job_name_demo, \"Gold\", \"SUCCESS\", demo_df.count())\n",
    "    print(f\">>> [SUCCESS] {job_name_demo} Refreshed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_pipeline_run(run_id, job_name_demo, \"Gold\", \"FAILED\", 0, str(e))\n",
    "    print(f\"!!! [ERROR] {job_name_demo} failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7842499125699272,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
