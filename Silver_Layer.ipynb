{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708af1ad-330f-47c6-925d-877687f4e776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Shared_Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00ee57b-b290-4381-9cfb-420e0a3631fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# 0. SETUP & UTILITIES\n",
    "# MAGIC %run ./Shared_Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc545ac-3e53-4cbe-a7b7-a22cc7395aeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. GET RUN ID & WIDGETS\n",
    "try:\n",
    "    run_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().runId().get()\n",
    "except:\n",
    "    run_id = \"manual_run\"\n",
    "\n",
    "dbutils.widgets.text(\"process_type\", \"all\")\n",
    "process_type = dbutils.widgets.get(\"process_type\")\n",
    "\n",
    "print(f\"Starting Silver Transformation for mode: {process_type} | Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebbb9f9b-00b6-4d2b-901a-c4b58de500e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PATH A: TRANSFORM USERS (SCD Type 2 Version)\n",
    "# ==========================================\n",
    "if process_type in [\"users\", \"all\"]:\n",
    "    job_name = \"Silver_Transform_Users\"\n",
    "    try:\n",
    "        print(f\">>> [START] {job_name}...\")\n",
    "        \n",
    "        # 1. READ FROM BRONZE\n",
    "        usersDF = spark.read.table(\"ecomm_data_project.bronze.users\")\n",
    "        \n",
    "        # 2. TRANSFORMATION LOGIC\n",
    "        users_silver = (\n",
    "            usersDF\n",
    "            .withColumn(\"countrycode\", F.upper(F.col(\"countrycode\")))\n",
    "            .withColumn(\n",
    "                \"gender\",\n",
    "                F.when(F.col(\"gender\").startswith(\"M\"), \"Male\")\n",
    "                .when(F.col(\"gender\").startswith(\"F\"), \"Female\")\n",
    "                .otherwise(\"Other\")\n",
    "            )\n",
    "            .withColumn(\"civilitytitle_clean\", F.regexp_replace(\"civilitytitle\", \"(Mme|Ms|Mrs)\", \"Ms\"))\n",
    "            .withColumn(\n",
    "                \"language_full\",\n",
    "                F.expr(\n",
    "                    \"CASE WHEN language = 'EN' THEN 'English' \"\n",
    "                    \"WHEN language = 'FR' THEN 'French' \"\n",
    "                    \"ELSE 'Other' END\"\n",
    "                )\n",
    "            )\n",
    "            .withColumn(\"account_age_years\", F.round(F.col(\"seniority\") / 365, 2))\n",
    "            .withColumn(\"productsSold\", F.col(\"productsSold\").cast(\"int\"))\n",
    "            .withColumn(\"productsWished\", F.col(\"productsWished\").cast(\"int\"))\n",
    "            .filter(F.col(\"identifierHash\").isNotNull())\n",
    "            .dropDuplicates([\"identifierHash\"])\n",
    "        )\n",
    "\n",
    "        # 3. ADD SCD TYPE 2 METADATA (Alignment with Perfect Shell)\n",
    "        # We add the columns we defined in the 'CREATE TABLE' statement\n",
    "        updates_df = users_silver.withColumn(\"is_current\", F.lit(True)) \\\n",
    "                                 .withColumn(\"effective_start_date\", F.current_timestamp()) \\\n",
    "                                 .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\"))\n",
    "\n",
    "        # 4. COLUMN ORDERING (Critical for Managed Tables)\n",
    "        # We must match the exact order of the columns in your '00_Setup_Infrastructure'\n",
    "        column_order = [\n",
    "        \"identifierHash\", \"countrycode\", \"gender\", \"civilitytitle_clean\", \n",
    "        \"language_full\", \"account_age_years\", \n",
    "        \"productsSold\", \"productsWished\", # <--- Add these two here!\n",
    "        \"is_current\", \"effective_start_date\", \"effective_end_date\"\n",
    "        ]\n",
    "        updates_df = updates_df.select(*column_order)\n",
    "\n",
    "        # 5. PERFORM MERGE / UPSERT LOGIC\n",
    "        target_table = \"ecomm_data_project.silver.users\"\n",
    "        \n",
    "        # We check if data already exists to handle history\n",
    "        if spark.catalog.tableExists(target_table) and spark.read.table(target_table).count() > 0:\n",
    "            print(\">>> Table has data. Processing history...\")\n",
    "            from delta.tables import DeltaTable\n",
    "            dt = DeltaTable.forName(spark, target_table)\n",
    "            \n",
    "            # Find records that have changed (e.g. country or gender changed)\n",
    "            # We only look at records that are CURRENTLY active (is_current = True)\n",
    "            changed_records = updates_df.alias(\"updates\").join(\n",
    "                spark.read.table(target_table).alias(\"target\"),\n",
    "                (F.col(\"updates.identifierHash\") == F.col(\"target.identifierHash\")) & \n",
    "                (F.col(\"target.is_current\") == True)\n",
    "            ).where(\n",
    "                \"updates.countrycode <> target.countrycode OR updates.gender <> target.gender\"\n",
    "            ).select(\"updates.identifierHash\")\n",
    "\n",
    "            # Expire the old records (Set is_current = False)\n",
    "            if changed_records.count() > 0:\n",
    "                print(f\">>> Expiring {changed_records.count()} historical records...\")\n",
    "                change_ids = [row['identifierHash'] for row in changed_records.collect()]\n",
    "                dt.update(\n",
    "                    condition = F.col(\"identifierHash\").isin(change_ids) & (F.col(\"is_current\") == True),\n",
    "                    set = { \"is_current\": \"false\", \"effective_end_date\": \"current_timestamp()\" }\n",
    "                )\n",
    "\n",
    "            # Append the new state\n",
    "            updates_df.write.format(\"delta\").mode(\"append\").saveAsTable(target_table)\n",
    "            \n",
    "        else:\n",
    "            # If table is empty, just do a clean initial load\n",
    "            print(\">>> Table is empty. Performing initial load...\")\n",
    "            updates_df.write.format(\"delta\").mode(\"append\").saveAsTable(target_table)\n",
    "        \n",
    "        # 6. LOG SUCCESS\n",
    "        final_count = spark.read.table(target_table).count()\n",
    "        log_pipeline_run(run_id, job_name, \"Silver\", \"SUCCESS\", final_count)\n",
    "        print(f\">>> [SUCCESS] {job_name} complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! [ERROR] {job_name} failed: {str(e)}\")\n",
    "        log_pipeline_run(run_id, job_name, \"Silver\", \"FAILED\", 0, str(e))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8531eee-82fa-4ee1-b089-20d9412c43ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PATH B: TRANSFORM REFERENCE DATA (Scheduled)\n",
    "# ==========================================\n",
    "if process_type in [\"reference\", \"all\"]:\n",
    "    job_name = \"Silver_Transform_Reference\"\n",
    "    try:\n",
    "        print(f\">>> [START] {job_name}...\")\n",
    "        total_records = 0\n",
    "        \n",
    "        # 1. Process Buyers\n",
    "        buyersDF = spark.read.table(\"ecomm_data_project.bronze.buyers\")\n",
    "        int_cols = ['buyers', 'topbuyers', 'femalebuyers', 'malebuyers', 'totalproductsbought']\n",
    "        \n",
    "        for c in int_cols:\n",
    "            buyersDF = buyersDF.withColumn(c, F.col(c).cast(IntegerType())).fillna(0, subset=[c])\n",
    "            \n",
    "        buyersDF = buyersDF.withColumn(\"country\", F.initcap(F.col(\"country\"))) \\\n",
    "                           .withColumn(\"female_to_male_ratio\", F.round(F.col(\"femalebuyers\") / (F.col(\"malebuyers\") + 1), 2))\n",
    "        \n",
    "        buyersDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecomm_data_project.silver.buyers\")\n",
    "        total_records += buyersDF.count()\n",
    "\n",
    "        # 2. Process Sellers & Countries\n",
    "        for table in [\"sellers\", \"countries\"]:\n",
    "            df = spark.read.table(f\"ecomm_data_project.bronze.{table}\")\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"ecomm_data_project.silver.{table}\")\n",
    "            total_records += df.count()\n",
    "\n",
    "        # Log Success\n",
    "        log_pipeline_run(run_id, job_name, \"Silver\", \"SUCCESS\", total_records)\n",
    "        print(f\">>> [SUCCESS] {job_name} complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_pipeline_run(run_id, job_name, \"Silver\", \"FAILED\", 0, str(e))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d05ac490-51ca-4959-a4e0-1c199ba4a50d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7456226426370351,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Layer",
   "widgets": {
    "process_type": {
     "currentValue": "all",
     "nuid": "ed1c39d5-4f2c-47d8-a58c-dcf8bef79d09",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "all",
      "label": null,
      "name": "process_type",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "all",
      "label": null,
      "name": "process_type",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
